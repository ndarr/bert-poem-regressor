{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BertPoems.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1A5uUE16Oe5cMhR_qUf8R5PvGryCnf2mI",
      "authorship_tag": "ABX9TyOKwyl33U8NgzWYMBPzGQvj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ndarr/bert-poem-regressor/blob/main/BertPoems.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwPVzz98FGQB"
      },
      "source": [
        "!pip install transformers\n",
        "!wget example.com #Replace with actual link to data \n",
        "!mkdir models\n",
        "!mkdir losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnW_OsuioyJ5"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for better reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch import tensor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Chooose between bert-large or distilbert-base\n",
        "bert = \"bert-large\"\n",
        "class RegressionModel(nn.Module):\n",
        "    def __init__(self, bertname):\n",
        "        super(RegressionModel, self).__init__()\n",
        "        self.bertname = bertname \n",
        "        self.bert = BertModel.from_pretrained(f\"{self.bertname}-uncased\")\n",
        "        self.emb_size = self.bert.config.hidden_size\n",
        "        self.batch_norm = nn.BatchNorm1d(self.emb_size)\n",
        "        self.drop_out = nn.Dropout()\n",
        "        self.dense1 = nn.Linear(self.emb_size, 1)\n",
        "        self.sig = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Get pooler output for whole sequence\n",
        "        y = self.bert(input_ids, attention_mask)['pooler_output']\n",
        "        \n",
        "        y = self.batch_norm(y)\n",
        "        y = self.drop_out(y)\n",
        "        \n",
        "        # hiddensize x 1 \n",
        "        y = self.dense1(y)\n",
        "\n",
        "        y = self.sig(y)\n",
        "\n",
        "        # Convert to float for better compatibility\n",
        "        y = y.float()\n",
        "        return y\n",
        "\n",
        "class PoemScoreDataset(Dataset):\n",
        "    def __init__(self, poems, targets, bertname):\n",
        "        assert len(poems) == len(targets)\n",
        "        self.bertname = bertname\n",
        "        self.poems = poems\n",
        "        self.targets = targets\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(f\"{self.bertname}-uncased\")\n",
        "        self.max_len = 80\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.poems)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        tokenized_poem = self.tokenizer.encode_plus(\n",
        "            self.poems[idx],\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt')\n",
        "        return tokenized_poem, tensor(self.targets[idx], dtype=torch.float)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9JUL3XEtzEO"
      },
      "source": [
        "from statistics import mean\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.optim import AdamW\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "columns = [\"bws_all\", \"bws_coherent\", \"bws_grammatical\", \"bws_melodious\", \"bws_moved\", \"bws_real\", \"bws_rhyming\", \"bws_readable\", \"bws_comprehensible\", \"bws_intense\",\"bws_liking\",\"crowdgppl_all\", \"crowdgppl_coherent\",\"crowdgppl_grammatical\",\"crowdgppl_melodious\",\"crowdgppl_moved\",\"crowdgppl_real\",\"crowdgppl_rhyming\",\"crowdgppl_readable\",\"crowdgppl_comprehensible\",\"crowdgppl_intense\",\"crowdgppl_liking\"]\n",
        "\n",
        "for target_column in columns:\n",
        "    targets = []\n",
        "    poems = []\n",
        "    print(f\"======{target_column}======\")\n",
        "    # Read scores from csv file\n",
        "    with open(\"normalized_scores.csv\") as f:\n",
        "        csv_reader = csv.reader(f)\n",
        "        header = next(csv_reader)\n",
        "        target_idx = header.index(target_column)\n",
        "        for row in csv_reader:\n",
        "            poem = row[0].replace(\"<br>\", \"\\n\")\n",
        "            poems.append(row[0])\n",
        "            targets.append(float(row[target_idx]))\n",
        "    device = \"cuda\"\n",
        "\n",
        "    model = RegressionModel(bertname=bert)\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    # Split poems and targets into train and test set\n",
        "    poems, test_poems, targets, test_targets = train_test_split(poems, targets, test_size=0.1)\n",
        "\n",
        "    dataset = PoemScoreDataset(poems, targets, bertname=bert)\n",
        "    test_dataset = PoemScoreDataset(test_poems, test_targets, bertname=bert)\n",
        "\n",
        "    optimizer = AdamW(model.parameters())\n",
        "\n",
        "    # L1 Loss as it worked best among the candidates\n",
        "    loss_fn = nn.L1Loss()\n",
        "    # Create dataloaders with their respective data\n",
        "    dataloader = DataLoader(dataset, batch_size=24, shuffle=False)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=24, shuffle=False)\n",
        "\n",
        "\n",
        "    epochs = 10\n",
        "    # Dictionary with losses for each epoch\n",
        "    epoch_losses = {}\n",
        "    epoch_test_losses = {}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        tqdm_loader = tqdm(dataloader)\n",
        "        losses = []\n",
        "\n",
        "        # Iterate over training batches\n",
        "        for input_, targets_ in tqdm_loader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Get input ids and attention mask from BERT tokenized poem\n",
        "            input_ids = input_['input_ids'].squeeze(1).to(device)\n",
        "            attention_mask = input_['attention_mask'].squeeze(1).to(device)\n",
        "\n",
        "            pred = model(input_ids, attention_mask).squeeze()\n",
        "            \n",
        "            targets_ = targets_.to(device)\n",
        "            loss = loss_fn(pred, targets_)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            losses.append(loss.item())\n",
        "            tqdm_loader.set_description(f\"Loss: {round(mean(losses),4)}\")\n",
        "        \n",
        "        epoch_losses[epoch] = mean(losses)\n",
        "        test_losses = []\n",
        "        model.eval()\n",
        "        # Test on testing data set for generalization capability\n",
        "        for input_, targets_ in test_dataloader:\n",
        "            input_ids = input_['input_ids'].squeeze(1).to(device)\n",
        "            attention_mask = input_['attention_mask'].squeeze(1).to(device)\n",
        "            pred = model(input_ids, attention_mask).squeeze()\n",
        "            targets_ = targets_.to(device)\n",
        "            loss = loss_fn(pred, targets_)\n",
        "            test_losses.append(loss.item())\n",
        "        \n",
        "        print(f\"Test loss epoch {epoch}:  {mean(test_losses)}\")\n",
        "        epoch_test_losses[epoch] = mean(test_losses)\n",
        "\n",
        "\n",
        "    # Save the current model \n",
        "    torch.save(model.state_dict(), f\"models/{bert}_model_{target_column}.pt\")\n",
        "    \n",
        "    # Save losses into file \n",
        "    loss_file = open(f\"losses/{bert}_model_losses_{target_column}.txt\",\"w+\")\n",
        "    loss_file.write(str(epoch_losses))\n",
        "    loss_file.close()\n",
        "\n",
        "    # Save test losses into file\n",
        "    test_loss_file = open(f\"losses/{bert}_test_losses_{target_column}.txt\",\"w+\")\n",
        "    test_loss_file.write(str(epoch_test_losses))\n",
        "    test_loss_file.close()\n",
        "\n",
        "    # Clear memory as far as possible for next iteration\n",
        "    del loss_file\n",
        "    del test_loss_file\n",
        "    del dataset\n",
        "    del dataloader\n",
        "    del optimizer\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}